{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8807600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0 status: 403\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error 403: {\"code\":1,\"message\":\"You need to be logged in to search listings\"}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ Skipped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mskipped\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m listings due to missing fields\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCh_7H2yKeV9_28fWemiPawkaL1Nic9N3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 71\u001b[0m listings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fetch_filtered_listings(API_KEY)\n\u001b[0;32m     72\u001b[0m save_to_csv(listings, OUTPUT_FILE)\n",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m, in \u001b[0;36mfetch_filtered_listings\u001b[1;34m(api_key, pages)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     31\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mtext()\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     35\u001b[0m batch \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "\u001b[1;31mException\u001b[0m: Error 403: {\"code\":1,\"message\":\"You need to be logged in to search listings\"}\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import csv\n",
    "\n",
    "OUTPUT_FILE = \"mil_spec_under_10.csv\"\n",
    "\n",
    "async def fetch_filtered_listings(api_key, pages=1):\n",
    "    headers = {\n",
    "        \"Authorization\": api_key,\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"User-Agent\": \"CSFloat-TradeUp-Script\"\n",
    "    }\n",
    "\n",
    "    listings = []\n",
    "\n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        for page in range(pages):\n",
    "            url = (\n",
    "                \"https://csfloat.com/api/v1/listings\"\n",
    "                f\"?limit=50&page={page}&sort_by=lowest_price\"\n",
    "                \"&type=buynow&rarity=3&max_price=1000\"\n",
    "                #\"&category=1&def_index=1,2,3,4,7,8,9,10,11,13,14,16,17,19,23,24,25,26,27,28,29,30,32,33,34,35,36,38,39,40,60,61,63,64\"\n",
    "            )\n",
    "            async with session.get(url) as resp:\n",
    "                print(f\"Page {page} status: {resp.status}\")\n",
    "                if resp.status == 429:\n",
    "                    print(\"Rate limited. Waiting 30s...\")\n",
    "                    await asyncio.sleep(30)\n",
    "                    continue\n",
    "                if resp.status != 200:\n",
    "                    text = await resp.text()\n",
    "                    raise Exception(f\"Error {resp.status}: {text}\")\n",
    "\n",
    "                data = await resp.json()\n",
    "                batch = data.get(\"data\", [])\n",
    "                if not batch:\n",
    "                    break\n",
    "                listings.extend(batch)\n",
    "                await asyncio.sleep(5)\n",
    "\n",
    "    return listings\n",
    "\n",
    "def save_to_csv(listings, filename):\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"market_hash_name\", \"collection\", \"rarity\", \"price\"])\n",
    "        saved, skipped = 0, 0\n",
    "\n",
    "        for l in listings:\n",
    "            \n",
    "            if (\n",
    "                item.get(\"type\") != \"skin\"\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            item = l.get(\"item\", {})\n",
    "            name = item.get(\"market_hash_name\")\n",
    "            coll = item.get(\"collection\")\n",
    "            rarity = item.get(\"rarity\")\n",
    "            price = l.get(\"price\")\n",
    "            if name and coll and rarity is not None and price is not None:\n",
    "                writer.writerow([name, coll, rarity, price])\n",
    "                saved += 1\n",
    "            else:\n",
    "                skipped += 1\n",
    "\n",
    "    print(f\"✅ Saved {saved} listings to {filename}\")\n",
    "    print(f\"⚠️ Skipped {skipped} listings due to missing fields\")\n",
    "\n",
    "API_KEY = \"Ch_7H2yKeV9_28fWemiPawkaL1Nic9N3\"\n",
    "listings = await fetch_filtered_listings(API_KEY)\n",
    "save_to_csv(listings, OUTPUT_FILE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93bd7c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Profitable Trade-Ups:\n"
     ]
    }
   ],
   "source": [
    "# analyze_tradeups_from_csv.py\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "INPUT_FILE = \"csfloat_listings.csv\"\n",
    "\n",
    "RARITY_ORDER = [\"consumer\", \"industrial\", \"mil-spec\", \"restricted\", \"classified\", \"covert\"]\n",
    "\n",
    "def next_rarity(rarity):\n",
    "    try:\n",
    "        return RARITY_ORDER[RARITY_ORDER.index(rarity) + 1]\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "def load_listings_from_csv(filename):\n",
    "    listings = []\n",
    "    with open(filename, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                listings.append({\n",
    "                    \"market_hash_name\": row[\"market_hash_name\"],\n",
    "                    \"collection\": row[\"collection\"],\n",
    "                    \"rarity\": row[\"rarity\"],\n",
    "                    \"price\": int(row[\"price\"])  # stored in cents\n",
    "                })\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return listings\n",
    "\n",
    "def group_by_collection_and_rarity(listings):\n",
    "    grouped = defaultdict(lambda: defaultdict(list))\n",
    "    for item in listings:\n",
    "        grouped[item[\"collection\"]][item[\"rarity\"]].append(item)\n",
    "    return grouped\n",
    "\n",
    "def analyze_tradeups(grouped_data):\n",
    "    tradeups = []\n",
    "    for collection, rarities in grouped_data.items():\n",
    "        for rarity in rarities:\n",
    "            if len(rarities[rarity]) < 10:\n",
    "                continue\n",
    "            next_rar = next_rarity(rarity)\n",
    "            if not next_rar or next_rar not in rarities:\n",
    "                continue\n",
    "\n",
    "            fillers = sorted(rarities[rarity], key=lambda x: x[\"price\"])[:10]\n",
    "            total_cost = sum(f[\"price\"] for f in fillers) / 100\n",
    "            outputs = rarities[next_rar]\n",
    "            prob = 1 / len(outputs)\n",
    "            ev = sum(prob * (o[\"price\"] / 100) for o in outputs)\n",
    "            roi = ev / total_cost\n",
    "\n",
    "            tradeups.append({\n",
    "                \"collection\": collection,\n",
    "                \"rarity\": rarity,\n",
    "                \"next_rarity\": next_rar,\n",
    "                \"total_cost\": total_cost,\n",
    "                \"expected_value\": ev,\n",
    "                \"roi\": roi,\n",
    "                \"output_count\": len(outputs),\n",
    "                \"output_skins\": [o[\"market_hash_name\"] for o in outputs]\n",
    "            })\n",
    "    return sorted(tradeups, key=lambda x: x[\"roi\"], reverse=True)\n",
    "\n",
    "listings = load_listings_from_csv(INPUT_FILE)\n",
    "grouped = group_by_collection_and_rarity(listings)\n",
    "tradeups = analyze_tradeups(grouped)\n",
    "\n",
    "print(\"\\nTop Profitable Trade-Ups:\")\n",
    "for i, t in enumerate(tradeups[:10], 1):\n",
    "    print(f\"\\n#{i} | Collection: {t['collection']}, Rarity: {t['rarity']} → {t['next_rarity']}\")\n",
    "    print(f\"Cost: ${t['total_cost']:.2f}, EV: ${t['expected_value']:.2f}, ROI: {t['roi']:.2f}, Outputs: {t['output_count']}\")\n",
    "    for out in t['output_skins']:\n",
    "        print(f\"  - {out}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18561a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiodns in c:\\users\\jiruszde\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: pycares>=4.9.0 in c:\\users\\jiruszde\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiodns) (4.9.0)\n",
      "Requirement already satisfied: cffi>=1.5.0 in c:\\users\\jiruszde\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pycares>=4.9.0->aiodns) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jiruszde\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.5.0->pycares>=4.9.0->aiodns) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c952f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "import aiodns\n",
    "print(aiodns.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c8ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
